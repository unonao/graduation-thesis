\documentclass[senior,final,11pt]{iscs-thesis}
\usepackage{amsmath,amssymb} % mathbb
\usepackage{amsmath,amsthm} % theorem
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]
% 論文の種類とフォントサイズをオプションに
%-------------------
\etitle{Analysis and Quantitative Evaluation of Large Networks Using Spectral Distributions}
\jtitle{スペクトル分布を用いた大規模ネットワークの分析と定量的評価について}
%
\eauthor{Naoki Murakami}
\jauthor{村上直輝}
\esupervisor{Hiroshi Imai}
\jsupervisor{今井浩}
\supervisortitle{Professor} % Professor, etc.
\date{January 29, 2021}
%-------------------
\begin{document}
\begin{eabstract}
    To analyze the properties and structure of a network, one of the very important tools is the spectrum, or eigenvalue distribution. However, since computing the full eigendecomposition is expensive, large networks have been analyzed by using only part of the spectrum. Recently, a method for calculating the entire eigenvalue distribution has been developed, but the analysis and application of the spectrum have not yet been deeply considered.

    In this paper, we firstly discuss the quantitative evaluation of spectra and its applications. For quantitative evaluation and comparison of spectral densities, we suggest applying the concepts of divergence and metric of discrete probability distributions to spectral distributions. This allows an intuitive comparison based on the structure of networks. As an application, a fast search algorithm for similar spectra using cosine distance is also discussed. Secondly, we analyze the distribution of the overall spectrum of theoretical graphs and real-world networks. Then, we discuss the relation between the network properties and the spectrum. As a result, for sparse networks, there is a large correspondence between the local structure of the network, called a motif, and the distribution of eigenvalues. In addition, we find that a graph in which vertices connected to neighbors have a sharp spectrum found in complex networks. Furthermore, we illustrate that graphs with vertices connected randomly have the semicircular spectra found in Erdős-Rényi random graphs.

    This study explains a quantitative evaluation and application of spectra and shows that the shape of the spectra can give us information about the network properties. This means that the spectral analysis is an effective tool for clarifying the network properties.


\end{eabstract}
\begin{jabstract}
    ネットワークの性質や構造を分析する上で、スペクトルは非常に重要な指標となり得る。しかし、大規模ネットワークについては、計算量の問題からスペクトルの一部を利用した分析しか行われていなかった。近年スペクトル全体を計算する手法が考案されたが、スペクトルの分析や応用などはまだ深く考えられていない。

    本研究では、はじめにスペクトルの定量的評価とその応用方法について議論する。スペクトル分布の定量的な評価や比較のために、離散確率分布に対するダイバージェンスや距離の概念を流用することを提案する。これにより、ネットワークの構造に基づいた直感的な比較が可能になる。応用方法として cosine distance を用いた類似のスペクトルを高速に検索する手法についても言及する。 次に、理論的に得られるグラフと実世界のネットワークのスペクトル全体の分布を分析し、ネットワークの性質とスペクトルの対応について考察する。 結果として、平均次数が小さい疎なネットワークについては、モチーフと呼ばれるネットワークの局所的な構造と固有値の分布が大きく対応していることが分かった。 また、近くの頂点同士が隣接するようなグラフは、複雑ネットワークに見られるような鋭いスペクトルを持つ。さらに、頂点同士の隣接にランダム性があるグラフは、Erdős-Rényi ランダムグラフに見られる半円状のスペクトルになることを例示する。

    本研究はスペクトルの定量的評価や応用法について提案するとともに、スペクトルの形状からネットワークの性質についての情報が得られることを示した。これによりスペクトルの分析がネットワークの性質解明に有効な手段であることがわかる。
\end{jabstract}
\maketitle

\begin{acknowledge}
    First of all, I really appreciate supports by Prof. Imai to proceed with my
    research, to speak in seminar, and to write the thesis. His helpful advice from insight has tought me how to work on research. I am also grateful to Assist. Prof. Hiraishi for giving me suggestions and taking care of me. Finally, I would like to thank all members of Imai Laboratry. When
    I got stuck in my research, they always gave me helpful technical advice.
\end{acknowledge}

\frontmatter %% 前付け
\tableofcontents % 目次
%\listoffigures % 図目次
%\listoftables % 表目次
%\lstlistoflistings % ソースコード目次
%-------------------
\mainmatter %% 本文

\chapter{Introduction}
\section{Complex Network}
\section{von Neumann entropy of Network}

The Kullback-Leibler Divergence \cite{kullback1951information} is the most popular divergence.
However, the KLD is assimmetric.
Several symmetrizations \cite{nielsen2019jensen} were proposed including the jeffreys divergence \cite{jeffreys1946invariant} and the Jensen-Shannon Divergence \cite{lin1991divergence}.

The square root of JS Divergence is called Jensen-Shannnon distance which has been proved to be a valid distance metric \cite{endres2003new}.

Von Neumann entropy,
Quantum Relative entropy,
Quentum Jensen-Shannon divergence \cite{briet2009properties, lamberti2008metric}

Inspired by quantum information theory, the concept of von Neumann graph entropy(VNGE) was introduced \cite{braunstein2006laplacian}.

Some approximate algorithms have been developed to compute Von Neumann Graph Entropy\cite{chen2019fast,tsitsulin2020just}.

other network distance metric, NetLSD, was developed \cite{tsitsulin2018netlsd,tsitsulin2020just}.

Motif multiplicity corresponds to eigenvalues multiplicity \cite{mehatari2015effect,dong2019network}.

There are fast online search tool for DOS\cite{borysov2018online,geilhufe2018towards}

\chapter{Preliminaries}
\section{Matrix and Spectra}
Firstly, we define some concepts regarding matrix and its eigenvalues and prove some theorems which are used later.
\begin{definition}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a n-dimensional matrix $\mathbf{A}$. The spectrum $\sigma(\mathbf{A})$ of $\mathbf{A}$ is the set of its overall eigenvalues.
    \[\sigma(\mathbf{A}) := \{\lambda_i | i=0,1,...,n-1\}\]
\end{definition}
\begin{definition}
    The spectral radius $\rho(\mathbf{A})$ of a n-dimensional matrix $\mathbf{A}$ is the nonnegative number
    \[\rho(\mathbf{A}):=\sup_{\lambda \in \sigma(A)} |\lambda|.\]
\end{definition}

\begin{theorem}
    \label{power_eigenvalues}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a real symmetric matrix $\mathbf{A}$ and the eigendecomposition of $\mathbf{A}$ be $\mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. Then $\lambda_0^k, \lambda_2^k, ..., \lambda_{n-1}^k$ are eigenvalues of $\mathbf{A}^k$.
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathbf{A}^k &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T})^k \\
        &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}})^k &\text{$\mathbf{Q}$ is a orthogonal matrix}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^{-1}}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^T}
    \end{align*}
\end{proof}

\begin{theorem}
    Let the eigendecomposition of a real symmetric matrix $\mathbf{A}$ be $\mathbf{A}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. If $f(x)$ is given by $f(x)=\sum_{k=0}^{\infty} a_k x^k$ with radius of convergence greater than the spectral radius $\rho(\mathbf{A})$ and $f(\mathbf{A})$ is defined by $f(\mathbf{A})=\sum_{k=0}^{\infty} a_k \mathbf{A}^k$, then
    \[f(\mathbf{A}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T.\]
\end{theorem}
\begin{proof}
    $\mathbf{\Lambda}$ is a diagonal matrix. Therefore $f(\mathbf{\Lambda})$ is easy to calculate:
    \[
    [f(\mathbf{\Lambda})]_{ij} = \begin{cases}
        f(\lambda_i) & (i=j) \\
        0 & (otherwise)
        \end{cases}
    \]
    By applying theorem \ref{power_eigenvalues} to each degree, we can get $f(\mathbf{A}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T$.
\end{proof}

\section{Laplacian Matrix}

\section{Stochastic Trace Estimator}
For trace estimation of large implicit matrices, we can use randomized estimator described by Hunchinson \cite{hutchinson1989stochastic, adams2018estimating}.
\begin{theorem}
    \label{Hunchinson}
    (Proposition 4.1 \cite{adams2018estimating}) Let ${\mathbf A} \in \mathbb{R}^{n\times n}$ be a square matrix and $\mathbf{x} \in \mathbb{R}^n$ be a random vector such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$. Then $\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{A}\mathbf{x}] = \mathrm{tr}(\mathbf{A})$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{A}\mathbf{x}] &= \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{x}^{T}\mathbf{A}\mathbf{x})] \\
          &=  \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{A}\mathbf{x}\mathbf{x}^{T})]  &\text{invariance to cyclic permutation} \\
          &= \mathrm{tr}(\mathbb{E}_{p(\mathbf{x})}[\mathbf{A}\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of trace}  \\
          &= \mathrm{tr}(\mathbf{A}\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of expectation}  \\
          &= \mathrm{tr}(\mathbf{A})
    \end{align*}
\end{proof}

The requirement $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$ for the random vector  $\mathbf{x}$ can be easily satisfied. For example, paractical choices of $p(\mathbf{x})$ include Ramemacher or standard normal distributions with zero mean and one variance.

The estimation method of $\mathrm{tr}(f(\mathbf{A}))$ is based on Theorem \ref{Hunchinson}, which we apply in our setting for the trace of power of matrix $f({\mathbf L})={\mathbf L}^k$ or $f({\mathbfcal{L}})={\mathbfcal L}^k$. The following corollary holds.
\begin{corollary}
    \label{approxtrace}
    If $f(\mathbf{A}) \in \mathbb{R}^{n\times n}$ is a square matrix, and $\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{n_v} \in \mathbb{R}^n$ are random vectors drawn from a distribution $p(\mathbf{x})$ such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{A})) =\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}f(\mathbf{A})\mathbf{x}] \approx \frac{1}{n_v}\sum_{i=1}^{n_v} \mathbf{x}_i^{T}f(\mathbf{A})\mathbf{x}_i
    \end{align*}
\end{corollary}

Unit vectors $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ can be used instead of $\mathbf{x}_i$. We can rewrite the formula of Corollary \ref{approxtrace}.
\begin{theorem}
    \label{approxtraceUniformed}
    Let $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ be a unit vector, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{A})) \approx \frac{n}{n_v}\sum_{i=1}^{n_v} \mathbf{z}_i^{T}f(\mathbf{A})\mathbf{z}_i
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathrm{tr}(f(\mathbf{A})) &\approx \frac{1}{n_v}\sum_{i=1}^{n_v} \mathbf{x}_i^{T}f(\mathbf{A})\mathbf{x}_i \\
        &= \frac{1}{n_v}\sum_{i=1}^{n_v} \|\mathbf{x}_i\|_2^2 \cdot \mathbf{z}_i^{T}f(\mathbf{A})\mathbf{z}_i \\
        &\approx \frac{1}{n_v}\sum_{i=1}^{n_v} n \cdot \mathbf{z}_i^{T}f(\mathbf{A})\mathbf{z}_i &\text{$ \mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$}\\
        &= \frac{n}{n_v}\sum_{i=1}^{n_v} \mathbf{z}_i^{T}f(\mathbf{A})\mathbf{z}_i
    \end{align*}
\end{proof}

For Randemacher vectors, $\|\mathbf{x}_i\|_2^2$ is always $n$. For other random vectors, $\mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$ as long as $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$.

For accurate computation, $n_v$ should approaches infinity. However  using large $n_v$ is expensive. For practical purposes, we can use small $n_v$. The convergence rate of such estimator was studied by Avron and Toledo\cite{avron2011randomized}. Hence, we only need to compute $\mathbf{z}_i^{T}f(\mathbf{A})\mathbf{z}_i$.

\section{Stochastic Lanczos Quadrature(SLQ)}
There is a method called stochastic Lanczos quadrature(SLQ) \cite{ubaru2017fast}, for approximate computing of the trace of functions of large matrices. Firstly, the stochastic trace estimator is used for approximating the trace. Next, the bilinear form $\mathbf{z}^{T}f(\mathbf{A})\mathbf{z}$ is reformed as a Riemann-Stieltjes integral and approximated by applying Gauss quadrature rule. Then, the Lanczos algorithm is applied for calculation.

The stochastic trace estimator has been explained in the previous section.



\chapter{the Sum of Powers of Network Eigenvalues}
\section{General Notation}

Let $G=(V,E,w)$ be a wighted undirected graph where $V=(v_1, v_2, ..., v_n)$ is a set of vertices, $E \subseteq V\times V$ is a set of undirected edges, and $w: E \rightarrow \mathbb{R}$ is a weight function. The adjacency matrix ${\mathbf A}$ is a $n \times n$ matrix of $G$, (i.e., $ A_{ij}= w((v_i ,u_i))$ ).
\begin{table}[htb]
    \begin{center}
      \begin{tabular}{l r}
        \bf{Symbol} & \bf{Description} \\ \hline
        $n \in \mathbb{N}$ & $|V|$:the number of vertices \\
        $m \in \mathbb{N}$ & $|E|$:the number of edges \\ \hline
      \end{tabular}
      \caption{Summary of notion used}
      \label{tab:notion}
    \end{center}
  \end{table}

\chapter{Conclusion}

%-------------------
\bibliographystyle{plain} % 参考文献
\bibliography{myref} %
%-------------------
\end{document}
