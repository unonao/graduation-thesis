\documentclass[senior,final,11pt]{iscs-thesis}
\usepackage{amsmath,amssymb} % mathbb
\usepackage{amsmath,amsthm} % theorem
\usepackage{algorithm,algorithmic}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]
% 論文の種類とフォントサイズをオプションに
%-------------------
\etitle{Analysis and Quantitative Evaluation of Large Networks Using Spectral Distributions}
\jtitle{スペクトル分布を用いた大規模ネットワークの分析と定量的評価について}
%
\eauthor{Naoki Murakami}
\jauthor{村上直輝}
\esupervisor{Hiroshi Imai}
\jsupervisor{今井浩}
\supervisortitle{Professor} % Professor, etc.
\date{January 29, 2021}
%-------------------
\begin{document}
\begin{eabstract}
    To analyze the properties and structure of a network, one of the very important tools is the spectrum, or eigenvalue distribution. However, since computing the full eigendecomposition is expensive, large networks have been analyzed by using only part of the spectrum. Recently, a method for calculating the entire eigenvalue distribution has been developed, but the analysis and application of the spectrum have not yet been deeply considered.

    In this paper, we firstly discuss the quantitative evaluation of spectra and its applications. For quantitative evaluation and comparison of spectral densities, we suggest applying the concepts of divergence and metric of discrete probability distributions to spectral distributions. This allows an intuitive comparison based on the structure of networks. As an application, a fast search algorithm for similar spectra using cosine distance is also discussed. Secondly, we analyze the distribution of the overall spectrum of theoretical graphs and real-world networks. Then, we discuss the relation between the network properties and the spectrum. As a result, for sparse networks, there is a large correspondence between the local structure of the network, called a motif, and the distribution of eigenvalues. In addition, we find that a graph in which vertices connected to neighbors have a sharp spectrum found in complex networks. Furthermore, we illustrate that graphs with vertices connected randomly have the semicircular spectra found in Erdős-Rényi random graphs.

    This study explains a quantitative evaluation and application of spectra and shows that the shape of the spectra can give us information about the network properties. This means that the spectral analysis is an effective tool for clarifying the network properties.


\end{eabstract}
\begin{jabstract}
    ネットワークの性質や構造を分析する上で、スペクトルは非常に重要な指標となり得る。しかし、大規模ネットワークについては、計算量の問題からスペクトルの一部を利用した分析しか行われていなかった。近年スペクトル全体を計算する手法が考案されたが、スペクトルの分析や応用などはまだ深く考えられていない。

    本研究では、はじめにスペクトルの定量的評価とその応用方法について議論する。スペクトル分布の定量的な評価や比較のために、離散確率分布に対するダイバージェンスや距離の概念を流用することを提案する。これにより、ネットワークの構造に基づいた直感的な比較が可能になる。応用方法として cosine distance を用いた類似のスペクトルを高速に検索する手法についても言及する。 次に、理論的に得られるグラフと実世界のネットワークのスペクトル全体の分布を分析し、ネットワークの性質とスペクトルの対応について考察する。 結果として、平均次数が小さい疎なネットワークについては、モチーフと呼ばれるネットワークの局所的な構造と固有値の分布が大きく対応していることが分かった。 また、近くの頂点同士が隣接するようなグラフは、複雑ネットワークに見られるような鋭いスペクトルを持つ。さらに、頂点同士の隣接にランダム性があるグラフは、Erdős-Rényi ランダムグラフに見られる半円状のスペクトルになることを例示する。

    本研究はスペクトルの定量的評価や応用法について提案するとともに、スペクトルの形状からネットワークの性質についての情報が得られることを示した。これによりスペクトルの分析がネットワークの性質解明に有効な手段であることがわかる。
\end{jabstract}
\maketitle

\begin{acknowledge}
    First of all, I really appreciate supports by Prof. Imai to proceed with my
    research, to speak in seminar, and to write the thesis. His helpful advice from insight has tought me how to work on research. I am also grateful to Assist. Prof. Hiraishi for giving me suggestions and taking care of me. Finally, I would like to thank all members of Imai Laboratry. When
    I got stuck in my research, they always gave me helpful technical advice.
\end{acknowledge}

\frontmatter %% 前付け
\tableofcontents % 目次
%\listoffigures % 図目次
%\listoftables % 表目次
%\lstlistoflistings % ソースコード目次
%-------------------
\mainmatter %% 本文

\chapter{Introduction}
\section{Complex Network}
\section{von Neumann entropy of Network}

The Kullback-Leibler Divergence \cite{kullback1951information} is the most popular divergence.
However, the KLD is assimmetric.
Several symmetrizations \cite{nielsen2019jensen} were proposed including the jeffreys divergence \cite{jeffreys1946invariant} and the Jensen-Shannon Divergence \cite{lin1991divergence}.

The square root of JS Divergence is called Jensen-Shannnon distance which has been proved to be a valid distance metric \cite{endres2003new}.

Von Neumann entropy,
Quantum Relative entropy,
Quentum Jensen-Shannon divergence \cite{briet2009properties, lamberti2008metric}

Inspired by quantum information theory, the concept of von Neumann graph entropy(VNGE) was introduced \cite{braunstein2006laplacian}.

Some approximate algorithms have been developed to compute Von Neumann Graph Entropy\cite{chen2019fast,tsitsulin2020just}.

other network distance metric, NetLSD, was developed \cite{tsitsulin2018netlsd,tsitsulin2020just}.

Motif multiplicity corresponds to eigenvalues multiplicity \cite{mehatari2015effect,dong2019network}.

There are fast online search tool for DOS\cite{borysov2018online,geilhufe2018towards}

\chapter{Preliminaries}
\section{General Notation}

Let $G=(V,E,w)$ be a wighted undirected graph where $V=(v_0, v_2, ..., v_{n-1})$ is a set of vertices, $E \subseteq V\times V$ is a set of undirected edges, and $w: E \rightarrow \mathbb{R}$ is a weight function. The adjacency matrix ${\mathbf A}$ is a $n \times n$ matrix of $G$, (i.e., $ A_{ij}= w(v_i ,u_i)$ ).

The laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ are defined by the weight function $w$ and the degree $d_v:=\sum_{u} w(u,v)$.
\[
    L(u,v) = \begin{cases}
        d_v - w(u,v) & \text{if} u=v, \\
        - w(u,v) &  \text{if $u$ and $v$ are adjacent,} \\
        0   & \text{otherwise.}
        \end{cases}
\]
\[
    \mathcal{L}(u,v) = \begin{cases}
        1 - \frac{w(u,v)}{d_v} & \text{if} u=v, \\
        - \frac{w(u,v)}{\sqrt{d_u d_v}} &  \text{if $u$ and $v$ are adjacent,} \\
        0   & \text{otherwise.}
        \end{cases}
\]

Let ${\mathbf M}$ denote a real symmetric matrix that can be ${\mathbf A}$, ${\mathbf L}$, or ${\mathbfcal L}$.
\begin{table}[htb]
    \begin{center}
      \begin{tabular}{l l}
        \bf{Symbol} & \bf{Description} \\ \hline
        $n \in \mathbb{N}$ & $|V|$:the number of vertices \\
        $m \in \mathbb{N}$ & $|E|$:the number of edges \\ \hline
        ${\mathbf M} \in \mathbb{R}^{n\times n}$ & Real symmetric matrix which can be ${\mathbf A}$, ${\mathbf L}$, or ${\mathbfcal L}$\\
        ${\mathbf A} \in \mathbb{R}^{n\times n}$ & Adjacency matrix of a graph $G$ \\
        ${\mathbf L} \in \mathbb{R}^{n\times n}$ & Laplacian matrix of a graph $G$ \\
        ${\mathbfcal L} \in \mathbb{R}^{n\times n}$ & Normalized Laplacian matrix of a graph $G$ \\
        \hline
        $n_v \in \mathbb{R}^+$ & Number of SLQ random vectors \\
        $s \in \mathbb{R}^+$ & Number of Lanczos algorithm steps\\
        ${\mathbf x} \in \mathbb{R}^n$ & Random vector which satisfies $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$\\
        \hline
      \end{tabular}
      \caption{Summary of notion}
      \label{tab:notion}
    \end{center}
  \end{table}

\section{Laplacian Matrix and eigenvalues}
L

\section{Matrix and Spectra}
Firstly, we define some concepts regarding matrix and its eigenvalues and prove some theorems which are used later.
\begin{definition}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a n-dimensional matrix $\mathbf{M}$. The spectrum $\sigma(\mathbf{M})$ of $\mathbf{M}$ is the set of its overall eigenvalues.
    \[\sigma(\mathbf{M}) := \{\lambda_i | i=0,1,...,n-1\}\]
\end{definition}
\begin{definition}
    The spectral radius $\rho(\mathbf{M})$ of a n-dimensional matrix $\mathbf{M}$ is the nonnegative number
    \[\rho(\mathbf{M}):=\sup_{\lambda \in \sigma(A)} |\lambda|.\]
\end{definition}

\begin{theorem}
    \label{power_eigenvalues}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a real symmetric matrix $\mathbf{M}$ and the eigendecomposition of $\mathbf{M}$ be $\mathbf{M}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. Then $\lambda_0^k, \lambda_2^k, ..., \lambda_{n-1}^k$ are eigenvalues of $\mathbf{M}^k$.
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathbf{M}^k &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T})^k \\
        &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}})^k &\text{$\mathbf{Q}$ is a orthogonal matrix}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^{-1}}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^T}
    \end{align*}
\end{proof}

\begin{theorem}
    Let the eigendecomposition of a real symmetric matrix $\mathbf{M}$ be $\mathbf{M}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. If $f(x)$ is given by $f(x)=\sum_{k=0}^{\infty} a_k x^k$ with radius of convergence greater than the spectral radius $\rho(\mathbf{M})$ and $f(\mathbf{M})$ is defined by $f(\mathbf{M})=\sum_{k=0}^{\infty} a_k \mathbf{M}^k$, then
    \[f(\mathbf{M}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T.\]
\end{theorem}
\begin{proof}
    $\mathbf{\Lambda}$ is a diagonal matrix. Therefore $f(\mathbf{\Lambda})$ is easy to calculate:
    \[
    [f(\mathbf{\Lambda})]_{ij} = \begin{cases}
        f(\lambda_i) & (i=j) \\
        0 & (otherwise)
        \end{cases}
    \]
    By applying theorem \ref{power_eigenvalues} to each degree, we can get $f(\mathbf{M}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T$.
\end{proof}


\section{Stochastic Trace Estimator} \label{STE}
For trace estimation of large implicit matrices, we can use randomized estimator described by Hunchinson \cite{hutchinson1989stochastic, adams2018estimating}.
\begin{theorem}
    \label{Hunchinson}
    (Proposition 4.1 \cite{adams2018estimating}) Let ${\mathbf A} \in \mathbb{R}^{n\times n}$ be a square matrix and $\mathbf{x} \in \mathbb{R}^n$ be a random vector such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$. Then $\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{M}\mathbf{x}] = \mathrm{tr}(\mathbf{M})$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{M}\mathbf{x}] &= \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{x}^{T}\mathbf{M}\mathbf{x})] \\
          &=  \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{M}\mathbf{x}\mathbf{x}^{T})]  &\text{invariance to cyclic permutation} \\
          &= \mathrm{tr}(\mathbb{E}_{p(\mathbf{x})}[\mathbf{M}\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of trace}  \\
          &= \mathrm{tr}(\mathbf{M}\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of expectation}  \\
          &= \mathrm{tr}(\mathbf{M})
    \end{align*}
\end{proof}

The requirement $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$ for the random vector  $\mathbf{x}$ can be easily satisfied. For example, paractical choices of $p(\mathbf{x})$ include Ramemacher or standard normal distributions with zero mean and one variance.

The following corollary holds.

\begin{corollary}
    \label{approxtrace}
    If $f(\mathbf{M}) \in \mathbb{R}^{n\times n}$ is a square matrix, and $\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{n_v} \in \mathbb{R}^n$ are random vectors drawn from a distribution $p(\mathbf{x})$ such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) =\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}f(\mathbf{M})\mathbf{x}] \approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} \mathbf{x}_i^{T}f(\mathbf{M})\mathbf{x}_i
    \end{align*}
\end{corollary}

Unit vectors $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ can be used instead of $\mathbf{x}_i$. We can rewrite the formula of Corollary \ref{approxtrace}.
\begin{theorem}
    \label{approxtraceUniformed}
    Let $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ be a unit vector, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) &\approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} \mathbf{x}_i^{T}f(\mathbf{M})\mathbf{x}_i \\
        &= \frac{1}{n_v}\sum_{i=0}^{n_v-1} \|\mathbf{x}_i\|_2^2 \cdot \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i \\
        &\approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} n \cdot \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i &\text{$ \mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$}\\
        &= \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
    \end{align*}
\end{proof}

For Randemacher vectors, $\|\mathbf{x}_i\|_2^2$ is always $n$. For other random vectors, $\mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$ as long as $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$.

For accurate computation, $n_v$ should approaches infinity. However  using large $n_v$ is expensive. For practical purposes, we can use small $n_v$. The convergence rate of such estimator was studied by Avron and Toledo\cite{avron2011randomized}.


\section{Estimation of bilinear form $\mathbf{z}^T f(\mathbf{M}) \mathbf{z}$} \label{bilinear}
There are relationships between matrix, orthogonal polynomials, quadrature rules and the Lanczos algorithms. Gene H Golub and G\'erard Meurant compiled a book of previous research on such relationships and on estimation and bounds of bilinear form $\mathbf{u}^T f(\mathbf{M}) \mathbf{v}$ \cite{golub2009matrices}. In this paper, we focus on a special case that $\mathbf{u}=\mathbf{v}=\mathbf{z}$.

Firstly,the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ is reformed as a Riemann-Stieltjes integral and approximated by applying Gauss quadrature rule. Then, the Lanczos algorithm is applied for calculation.

The Riemann–Stieltjes integral is a generalization of the Riemann integral. It is defined to be the limit of a sum.
\begin{definition}
    A Riemann–Stieltjes integral of a real valued function $f$ of a real variable on the infinite interval $[a,b]$ with respect to a real function $\alpha$ is denoted by
    \[ \int_a^b f(x) d\alpha (x). \]
    This integral is defined to be the limit, as the length of the subinterval of the partition $\pi=\{a=x_0 < x_1 < ... < x_n = b\}$ goes to zero, of the approximating sum
    \[ \sum_{x_i \in \pi} f(c_i)(\alpha(x_{i+1}) - \alpha(x_i)).\]
    where $c_i$ is in the i-th subinterval $[x_i, x_{i+1}]$.
\end{definition}

\begin{theorem}
    gauss quadrature
\end{theorem}

Finally, we can reform and compute the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ using the Gauss quadrature rule.
\begin{align*}
    \mathbf{z}^{T}f(\mathbf{M})\mathbf{z} &= \mathbf{z}^{T}\mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T\mathbf{z}\\
    &= \sum_{j=0}^{n-1}f(\mathbf{\lambda}_i)\mu^2_j \\
    &= \int_a^b f(t) d\mu(t) &\text{Riemann–Stieltjes integral}\\
    &\approx \sum_{k=0}^{s-1} \omega_k f(\theta_k) &\text{the Gauss quadrature rule}
\end{align*}
where $\mu_j = [\mathbf{Q}^T\mathbf{z}]_j$ and $\mu(t)$ is a piecewise constant function defined as
\begin{align*}
    \mu(t) = \begin{cases}
        0 & \text{if $t<a=\lambda_0$} \\
        \sum_{j=0}^{i-1} \mu_j^2 & \text{if $\lambda_{i-1} \leq t < \lambda_i, i=1,\cdots, n-1$} \\
        \sum_{j=0}^{n-1} \mu_j^2 & \text{if $b=\lambda_{n-1}\leq t$}
      \end{cases}
\end{align*}
and $\{\omega_k\}$ are the weights and $\{\theta_k\}$ are the nodes of the s-point Gauss quadrature.

One choise for computing the nodes and the weights of the Gauss quadrature rule is the Lanczos algorithm. Let $\mathbf{M}$ be a real symmetric matrix, $\mathbf{w}_0$ be an arbitrary starting unit-vector, and ${\mathbfcal K}$ be the Krylov subspace spanning vectors $\{\mathbf{w}_0, \mathbf{M}\mathbf{w}_0, \cdots, \mathbf{M}^{s-1}\mathbf{w}_0\}$.
The output of the Lanczos algorithm are an $n \times s$ matrix $\mathbf{W}$ and an $s \times s$ tridiagonal matrix $\mathbf{T}$, such that $\mathbf{W}^T\mathbf{M}\mathbf{W} = \mathbf{T}$. $\mathbf{W}$ has the orthonormal columns:
\begin{align*}
    w_0 : \text{an initial unit-vector}\\
    w_k = p_k(\mathbf{M})w_0,~  k=1,\cdots,s-1
\end{align*}
where $p_k$ are the Lanczos polynomials which are orthogonal with respect to the measure $\mu(t)$.

Now, the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ is reformed as
\begin{align}
    &\mathbf{z}^{T}f(\mathbf{M})\mathbf{z} \approx  \sum_{k=0}^{s-1} \tau_k^2 f(\theta_k)\\
    &\tau_k = \mathbf{Y}_{0,k} = \mathbf{e}_1^T \mathbf{y}_k, ~ \theta_k= \Theta_{k,k}, ~ \mathbf{T}=\mathbf{Y}\mathbf{\Theta}\mathbf{Y}^T
\end{align}

\section{Stochastic Lanczos Quadrature(SLQ)}
There is a method called stochastic Lanczos quadrature(SLQ) \cite{ubaru2017fast}, for approximate computing of the trace of functions of large matrices. Firstly, the stochastic trace estimator is used for approximating the trace. Next, the estimation method we discussed in the section \ref{bilinear} is applied.

The stochastic trace estimator has been explained in the previous section.
\begin{align}
    \mathrm{tr}(f(\mathbf{M})) =\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}f(\mathbf{M})\mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
\end{align}

Now, we need to compute $\mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i$. By applying (2.1) to the stochastic trace estimator (2.3), we get Stochastic Lanczos Quadrature estimator:
\begin{align}
    \mathrm{tr}(f(\mathbf{M})) \approx \frac{n}{n_v} \sum_{i=0}^{n_v-1}\left(\sum_{k=0}^{s-1} (\tau_k^i)^2 f(\theta_k^i) \right)
\end{align}




\chapter{the Sum of Powers of Eigenvalues of Network (SPENet)}
Our main purpose is computing sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$ which we call SPENet. We propose two approximation algorithms (STE and SLQ) based on the stochastic trace estimator and the stochastic Lanczos quadrature. For naive algorithms, the computational complexity of SPENet using full eigendecomposition is $O(n^3)$, while for STE algorithm it is $O(m k n_v)$ and for SLQ algorithm it is $O((ms + ns^2) n_v)$.

Since the laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ have zero eigenvalues, we are interested in the case that index of power $k$ is positive. When we consider the case that ${\mathbf M} = {\mathbf A}$, $k$ must be positive integer or zero because ${\mathbf A}$ have negative eigenvalues.


\section{Algorithms}
\subsection{STE algorithm}
The first estimation method for SPENet is based on the stochastic trace estimator(STE), which we apply in our setting for the trace of power of matrix $f({\mathbf M})={\mathbf M}^k$. We are interested in the case where ${\mathbf M} = {\mathbf A}, {\mathbf L}, \text{or} {\mathbfcal L}$. In this algorithm, the index of power $k$ must be positve integer.

By using theorem \ref{power_eigenvalues} and \ref{approxtraceUniformed}, we can apply the stochastic trace estimator for computing sum of powers of k-th eigenvalues.
\begin{align}
    \sum_{i=0}^{n-1} \lambda_i^k &= \mathrm{tr}({\mathbf M}^k)  \nonumber\\
    &= \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}{\mathbf M}^k \mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i
\end{align}

The product of two matrices costs $O(n^3)$, but the product of a vector and a sparse matrix costs only $O(m)$. Therefore, when calculating $\mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i$, we just calculate the product of a vector and a matrix k times, and the inner product of a vector once from the left. From the above, the cost of the STE algorithm will be $O(mkn_v)$.


\subsection{SLQ algorithm}
The second estimation method for SPENet is based on the stochastic Lanczos quadrature(SLQ), which we apply in our setting for the trace of power of matrix $f({\mathbf M})={\mathbf M}^k$. We are interested in the case where ${\mathbf M} = {\mathbf A}, {\mathbf L}, \text{or} {\mathbfcal L}$. In this algorithm, the index of power $k$ must be positve real number.

By using theorem 1 and (2.4), we can estimate SPENet.
\begin{align}
    \sum_{i=0}^{n-1} \lambda_i^k &= \mathrm{tr}({\mathbf M}^k)  \nonumber\\
    &= \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}{\mathbf M}^k \mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i \nonumber\\
    &\approx \frac{n}{n_v} \sum_{i=0}^{n_v-1}\left(\sum_{j=0}^{s-1} (\tau_j)^2 (\theta_j)^k \right)
\end{align}
Since we apply s-step Lanczos algorithm for $\tau_k^i$ and $\theta_k^i$, the orthogonalization cost inside the Lanczos algorithm is $O(ns^2)$ and the cost of the product of a vector and a matrix s times in the Lanczos algorithm is $O(ms)$. From the above, all the computational cost of SLQ algorithm is $O((ms+ns^2)n_v)$.


\begin{algorithm}
    \caption{STE algorithm}
    \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE a real symmetric matrix ${\mathbf M} \in {\mathbb R}^{n\times n}$, a positive integer $k$, and $n_v$.
    \ENSURE  Approximate sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$
     \FOR {$i = 0$ to $n_v-1$}
     \STATE Generate a random vector $\mathbf{x}_i$ with the standard normal distribution.
     \STATE Form unit vector $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$.
     \STATE $\mathbf{v}_i^T \leftarrow \mathbf{z}_i^T $.
        \FOR {$j = 0$ to $k-1$}
        \STATE $\mathbf{v}_i^T \leftarrow \mathbf{v}_i^T {\mathbf M}$.
        \ENDFOR
     \STATE $\Gamma \leftarrow \Gamma + \mathbf{v}_i^T \mathbf{z}_i$
     \ENDFOR
    \STATE $\Gamma \leftarrow \frac{n}{n_v}\Gamma$
    \RETURN $\Gamma$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{SLQ algorithm}
    \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE a real symmetric matrix ${\mathbf M} \in {\mathbb R}^{n\times n}$, a positive real number $k$, and $n_v$.
    \ENSURE  Approximate sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$
     \FOR {$i = 0$ to $n_v-1$}
     \STATE Generate a random vector $\mathbf{x}_i$ with the standard normal distribution.
     \STATE Form unit vector $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$.
     \STATE $T \leftarrow \text{Lanczos($\mathbf{M}$,$\mathbf{z}_i$, s)}$.
     \STATE $[\mathbf{Y}, \mathbf{\Theta}] \leftarrow \text{eig}(T)$
     \STATE Compute $\tau_j = [e_1^T y_j]$ for $j=0, ..., s-1$
     \STATE $\Gamma \leftarrow \Gamma + \sum_{j=0}^{s-1} (\tau_j)^2 (\theta_j)^k$
     \ENDFOR
    \STATE $\Gamma \leftarrow \frac{n}{n_v}\Gamma$
    \RETURN $\Gamma$
    \end{algorithmic}
\end{algorithm}


\section{Conditions of $k$}
In SLQ algorithm, we can generalize the index of power $k$. If ${\mathbf M}$ is positive definite, we can use not only a positive integer but also a positive real number as $k$. This is because that $f(x)=x^k$ is analytic inside a closed interval $[\lambda_0, \lambda_{n-1}]$. However the function $f(x)=x^k$ have a singularity at zero and the laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ are positive semi-definite matrix, which have zero eigenvalues.

To overcome the issue mentioned above, we can use {\it shifting the spectrum} proposed by Shashanka Ubaru, Jie Chen, and Yousef Saad\cite{ubaru2017fast}. The idea is shifting the eigenvalues by replacing ${\mathbf M}$ with ${\mathbf M}+\delta{\mathbf I}$. The shifted matrix have the eigenvalues in the interval $[\lambda_0 + \delta, \lambda_{n-1}+ \delta]$. Hence we can obtaing the approximation error bounds of SLQ algorithm and find that SLQ algorithm is practically useful.

If $k$ is a positive real number and ${\mathbf M}={\mathbf A}$, $\sum_{i=0}^{n-1} \lambda_i^k$ is undefined because ${\mathbf A}$ have negative eigenvalues.

\begin{table}[htb]
    \begin{center}
      \begin{tabular}{l| c c c }
       & ${\mathbf A}$ & ${\mathbf L}$ & ${\mathbfcal L}$ \\ \hline
        $k\in \mathbb{N}_{\geq 0}$ & STE or SLQ  & STE or SLQ  & STE or SLQ \\
        $k\in \mathbb{R}_{\geq 0}$&  undefined  & SLQ  &  SLQ \\
      \end{tabular}
      \caption{Conditions under which the algorithms can be used}
      \label{tab:notion}
    \end{center}
  \end{table}

\section{Error bounds}
Suppose $k$ is a positive integer and Lanczos step size is $s$. When $2s-1$ is greater or equal to the degree of $f(x)$, the error of Gauss quadrature rule is zero.

\section{Experiments}
We evaluate computational time and variance of SPENet using STE and SLQ algorithm. Additionally, we evaluate its relative error against the exact computation of the eigenvalues where allowed by the graph size. We perform our experiments averating 10 times for all experiments unless stated otherwise.

\subsection{Parameter Settings}


\chapter{Conclusion}

%-------------------
\bibliographystyle{plain} % 参考文献
\bibliography{myref} %
%-------------------
\end{document}
