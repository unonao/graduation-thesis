\documentclass[senior,final,11pt]{iscs-thesis}
\usepackage{amsmath,amssymb} % mathbb
\usepackage{amsmath,amsthm} % theorem
\usepackage{algorithm,algorithmic}
\DeclareMathAlphabet\mathbfcal{OMS}{cmsy}{b}{n}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]
% 論文の種類とフォントサイズをオプションに
%-------------------
\etitle{Analysis and Approximation Algorithms for Large-Scale Networks Using All Eigenvalues}
\jtitle{全ての固有値を利用した大規模ネットワークの分析と近似アルゴリズム}
%
\eauthor{Naoki Murakami}
\jauthor{村上直輝}
\esupervisor{Hiroshi Imai}
\jsupervisor{今井浩}
\supervisortitle{Professor} % Professor, etc.
\date{January 29, 2021}
%-------------------
\begin{document}
\begin{eabstract}
    Spectral graph theory studies the relationship between the properties of graphs and eigenvalues. Spectra, or eigenvalues, are useful for analyzing the properties and structure of networks. However, since computing the full eigendecomposition is expensive, large networks have been analyzed using part of the spectrum. Research using the overall spectra of a large-scale network is still an underdeveloped field.

    Recently, a method to calculate the distribution of the entire spectrum has been devised, but since there are still few studies that visualize the spectral distribution of real-world networks, the correspondence between the spectral distribution and the properties of the network has not yet been considered deeply. In this study, we first discuss the quantitative evaluation of spectra and their application methods. For the quantitative evaluation and comparison of spectral distributions, we propose to use the concepts of divergence and distance for discrete probability distributions. As an application method, we also mention a method for fast retrieval of similar spectra using cosine distance. Next, we present the spectral distributions obtained from the synthesized graph and the real-world network. Next, we analyze the obtained spectral distributions and discuss the relationship between the network properties and the spectra. The results show that for sparse networks, the local structure of the network, called motifs, and the distribution of eigenvalues are related. Besides, graphs in which nearby vertices are adjacent to each other have a sharp spectrum, as seen in complex networks. Furthermore, we exemplify that many graphs where vertices are randomly adjacent to each other have a semicircular spectral distribution as seen in Erdős-Rényi random graphs.

    Next, we propose two approximation algorithms to compute the sum of powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$ that scale to large networks. The sum of powers of eigenvalues is an important property, and in recent years, research on related inequalities has been conducted. However, the conventional simple algorithms for calculating the sum of powers of eigenvalues do not scale to large graphs. Our proposed method makes it possible to calculate the sum of powers of eigenvalues for networks with millions or billions of vertices in a practical time. This makes it possible to use the sum of powers of eigenvalues as a new measure for the network.


\end{eabstract}
\begin{jabstract}
    Spectral graph theory ではグラフの性質と固有値の関係について研究されるなど、ネットワークの性質や構造を分析する上でスペクトルは非常に重要な指標となり得る。しかし、大規模ネットワークについては、計算量の問題からスペクトルの一部を利用した分析しか行われていなかった。大規模ネットワークの全てのスペクトルを利用した研究はまだ未発展の分野である。

    近年スペクトル全体の分布を計算する手法が考案されたが、実世界のネットワークのスペクトル分布を可視化した研究はまだ少ないため、スペクトル分布とネットワークの性質の対応関係についてはまだ深く考えられていない。本研究では、はじめにスペクトル分布の定量的評価とその応用方法について議論する。スペクトル分布の定量的な評価や比較のために、離散確率分布に対するダイバージェンスや距離の概念を流用することを提案する。応用方法として cosine distance を用いた類似のスペクトルを高速に検索する手法についても言及する。 次に、合成されたグラフと実世界のネットワークから得られたスペクトル分布を提示する。次に、得られたスペクトル分布を分析し、ネットワークの性質とスペクトルの関係について考察する。 結果として、疎なネットワークについては、モチーフと呼ばれるネットワークの局所的な構造と固有値の分布が関係していることが分かった。 また、近くの頂点同士が隣接するようなグラフは、複雑ネットワークに見られるような鋭いスペクトルを持つ。さらに、頂点同士がランダムに隣接する多くのグラフは、Erdős-Rényi ランダムグラフに見られる半円状のスペクトルになることを例示する。

    次に、本研究では、大規模ネットワークにスケールするような、固有値の累乗の合計値 $\sum_{i=0}^{n-1} \lambda_i^k$ をもとめる近似アルゴリズムを2つ提案する。固有値の累乗の合計値は重要な性質を持っており、近年は関係した不等式の研究なども行われている。しかし、固有値の累乗の合計値を計算する従来の単純なアルゴリズムでは大規模グラフにスケールしなかった。本研究の提案手法により実用的な時間で、数百万や数十億単位の頂点を持つネットワークについて固有値の累乗の合計値を計算することが可能になった。 これにより、ネットワークに対する新たな指標として、固有値の累乗の合計値というものが利用可能となる。

\end{jabstract}
\maketitle

\begin{acknowledge}
    First of all, I would like to express my gratitude to Professor Imai for his support in my research and writing this paper. His advice and insights were very helpful not only in the content of my research, but also in how to proceed with my research. I am also grateful to Assist. Prof. Hiraishi for taking care of me. Finally, I would like to thank all members of Imai Laboratry.
\end{acknowledge}

\frontmatter %% 前付け
\tableofcontents % 目次
%\listoffigures % 図目次
%\listoftables % 表目次
%\lstlistoflistings % ソースコード目次
%-------------------
\mainmatter %% 本文

\chapter{Introduction}
\section{Large-Scale Network}
Large-scale networks appear in many places in our daily lives. For example, IP address networks, map graphs, web graphs, and so on. Algorithms that have been used in the past do not scale to the huge networks that appear in the real world. Therefore, research is being done to develop algorithms that can be applied to large-scale networks and to elucidate the properties of large-scale networks.

\section{Spectra and Graphs}
Spectral graph theory is one powerful tool for understanding the properties of networks. This theory is based on the fact that the properties of a network and its eigenvalues, when expressed as a matrix, are interrelated. However, when considering eigenvalues for real-world networks, only a subset of the eigenvalues have been analyzed due to computational complexity issues, since the computational complexity of obtaining all the eigenvalues is $O(n^3)$ when the number of vertices is $n$. In recent years, approximation algorithms that use all eigenvalues for analysis have been proposed, but this is still an undeveloped field.

\subsection{Spectral Distributions}
Recently, an approximation algorithm for the distribution of eigenvalues that scales to large graphs has been proposed. This has made it possible to visualize the distribution of eigenvalues of various networks in the real world. However, a comprehensive analysis of the synthesized graphs and real-world networks has not yet been done.

\subsection{Spectral Distance???}

The Kullback-Leibler Divergence \cite{kullback1951information} is the most popular divergence.
However, the KLD is assimmetric.
Several symmetrizations \cite{nielsen2019jensen} were proposed including the jeffreys divergence \cite{jeffreys1946invariant} and the Jensen-Shannon Divergence \cite{lin1991divergence}.

The square root of JS Divergence is called Jensen-Shannnon distance which has been proved to be a valid distance metric \cite{endres2003new}.

Von Neumann entropy,
Quantum Relative entropy,
Quentum Jensen-Shannon divergence \cite{briet2009properties, lamberti2008metric}

Inspired by quantum information theory, the concept of von Neumann graph entropy(VNGE) was introduced \cite{braunstein2006laplacian}.

Some approximate algorithms have been developed to compute Von Neumann Graph Entropy\cite{chen2019fast,tsitsulin2020just}.

other network distance metric, NetLSD, was developed \cite{tsitsulin2018netlsd,tsitsulin2020just}.

Motif multiplicity corresponds to eigenvalues multiplicity \cite{mehatari2015effect,dong2019network}.

There are fast online search tool for DOS\cite{borysov2018online,geilhufe2018towards}

The eigenvalues of adjacency matrix and the number of closed walks are closely related\cite{butler2008eigenvalues}.

\subsection{Sum of Powers of Eigenvalues}

\section{Our Contributions}
We study the spectral distribution and the sum of powers of eigenvalues as an analysis using all eigenvalues. First, we visualize and comprehensively summarize the spectral distributions for many synthesized graphs and real-world networks, and discuss methods and applications for treating spectral distributions quantitatively. Second, we propose an approximation algorithm for the sum of powers of eigenvalues that scales to large networks. By applying methods such as stochastic trace estimator and stochastic Lanczos quadrature, the sum of powers of eigenvalues can be calculated in a practical time. Experimental results for the analysis of relative errors are also shown.

\chapter{Preliminaries}
\section{General Notation}

Let $G=(V,E,w)$ be a wighted undirected graph where $V=(v_0, v_2, ..., v_{n-1})$ is a set of vertices, $E \subseteq V\times V$ is a set of undirected edges, and $w: E \rightarrow \mathbb{R}$ is a weight function. The adjacency matrix ${\mathbf A}$ is a $n \times n$ matrix of $G$, (i.e., $ A_{ij}= w(v_i ,u_i)$ ).

The laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ are defined by the weight function $w$ and the degree $d_v:=\sum_{u} w(u,v)$.
\[
    L(u,v) = \begin{cases}
        d_v - w(u,v) & \text{if $u=v$}, \\
        - w(u,v) &  \text{if $u$ and $v$ are adjacent,} \\
        0   & \text{otherwise.}
        \end{cases}
\]
\[
    \mathcal{L}(u,v) = \begin{cases}
        1 - \frac{w(u,v)}{d_v} & \text{if $u=v$}, \\
        - \frac{w(u,v)}{\sqrt{d_u d_v}} &  \text{if $u$ and $v$ are adjacent,} \\
        0   & \text{otherwise.}
        \end{cases}
\]

Let ${\mathbf M}$ denote a real symmetric matrix that can be ${\mathbf A}$, ${\mathbf L}$, or ${\mathbfcal L}$.
\begin{table}[htb]
    \begin{center}
      \begin{tabular}{l l}
        \bf{Symbol} & \bf{Description} \\ \hline
        $n \in \mathbb{N}$ & $|V|$:the number of vertices \\
        $m \in \mathbb{N}$ & $|E|$:the number of edges \\ \hline
        ${\mathbf M} \in \mathbb{R}^{n\times n}$ & Real symmetric matrix which can be ${\mathbf A}$, ${\mathbf L}$, or ${\mathbfcal L}$\\
        ${\mathbf A} \in \mathbb{R}^{n\times n}$ & Adjacency matrix of a graph $G$ \\
        ${\mathbf L} \in \mathbb{R}^{n\times n}$ & Laplacian matrix of a graph $G$ \\
        ${\mathbfcal L} \in \mathbb{R}^{n\times n}$ & Normalized Laplacian matrix of a graph $G$ \\
        \hline
        $n_v \in \mathbb{R}^+$ & Number of SLQ random vectors \\
        $s \in \mathbb{R}^+$ & Number of Lanczos algorithm steps\\
        ${\mathbf x} \in \mathbb{R}^n$ & Random vector which satisfies $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$\\
        \hline
      \end{tabular}
      \caption{Summary of notion}
      \label{tab:notion}
    \end{center}
  \end{table}

\section{The Laplacian and Eigenvalues}
The relationship between graphs and eigenvalues has been studied in spectral graph theory. A particular focus has been on eigenvalues of normalized laplacian matrices, and Fan Chung has written a comprehensive textbook on the subject\cite{chung1997spectral}, but there are also studies on laplacian matrices.

Let ${\mathbf D}$ denote the diagonal matrix, such that
\begin{align*}
    \mathbf{D}(u,v) = \begin{cases}
        d_v & \text{if $u=v$}, \\
        0   & \text{otherwise.}
        \end{cases}
\end{align*}

There are some relationships among the adjacency matrix, laplacian matrix, and  normalized laplacian matrix.

By definitions, we can write
\begin{align*}
    \mathbfcal{L} &= \mathbf{D}^{-\frac{1}{2}}\mathbf{L}\mathbf{D}^{-\frac{1}{2}}\\
        &= \mathbf{I} - \mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}
\end{align*}


\begin{theorem}
    If G has no negative edges, $\mathbf{L}$ and $\mathbfcal{L}$ are positive semi-definite.
\end{theorem}
\begin{proof}
    For all $\mathbf{x} \in \mathbb{R}^n$,
    \begin{align*}
        \mathbf{x}^T \mathbf{L} \mathbf{x} = \sum_{(u,v) \in E} w(u,v)(x_u - x_v)^2 \geq 0
    \end{align*}
    \begin{align*}
        \mathbf{x}^T \mathbfcal{L} \mathbf{x} = \sum_{(u,v) \in E} w(u,v)\left(\frac{x_u}{\sqrt{d_u}} - \frac{x_v}{\sqrt{d_v}}\right)^2 \geq 0
    \end{align*}
\end{proof}

\section{Matrix and Spectra}
Firstly, we define some concepts regarding matrix and its eigenvalues and prove some theorems which are used later.
\begin{definition}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a n-dimensional matrix $\mathbf{M}$. The spectrum $\sigma(\mathbf{M})$ of $\mathbf{M}$ is the set of its overall eigenvalues.
    \[\sigma(\mathbf{M}) := \{\lambda_i | i=0,1,...,n-1\}\]
\end{definition}
\begin{definition}
    The spectral radius $\rho(\mathbf{M})$ of a n-dimensional matrix $\mathbf{M}$ is the nonnegative number
    \[\rho(\mathbf{M}):=\sup_{\lambda \in \sigma(A)} |\lambda|.\]
\end{definition}

\begin{theorem}
    \label{power_eigenvalues}
    Let $\lambda_0 \leq \lambda_2 \leq ... \leq \lambda_{n-1}$ be the eigenvalues of a real symmetric matrix $\mathbf{M}$ and the eigendecomposition of $\mathbf{M}$ be $\mathbf{M}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. Then $\lambda_0^k, \lambda_2^k, ..., \lambda_{n-1}^k$ are eigenvalues of $\mathbf{M}^k$.
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathbf{M}^k &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T})^k \\
        &= (\mathbf{\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^{-1}})^k &\text{$\mathbf{Q}$ is a orthogonal matrix}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^{-1}}\\
        &= \mathbf{\mathbf{Q}\mathbf{\Lambda}^k\mathbf{Q}^T}
    \end{align*}
\end{proof}

\begin{theorem}
    Let the eigendecomposition of a real symmetric matrix $\mathbf{M}$ be $\mathbf{M}=\mathbf{Q}\mathbf{\Lambda}\mathbf{Q}^T$. If $f(x)$ is given by $f(x)=\sum_{k=0}^{\infty} a_k x^k$ with radius of convergence greater than the spectral radius $\rho(\mathbf{M})$ and $f(\mathbf{M})$ is defined by $f(\mathbf{M})=\sum_{k=0}^{\infty} a_k \mathbf{M}^k$, then
    \[f(\mathbf{M}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T.\]
\end{theorem}
\begin{proof}
    $\mathbf{\Lambda}$ is a diagonal matrix. Therefore $f(\mathbf{\Lambda})$ is easy to calculate:
    \[
    [f(\mathbf{\Lambda})]_{ij} = \begin{cases}
        f(\lambda_i) & (i=j) \\
        0 & (otherwise)
        \end{cases}
    \]
    By applying theorem \ref{power_eigenvalues} to each degree, we can get $f(\mathbf{M}) = \mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T$.
\end{proof}

\section{Approximation Methods for Matrix Traces and Bilinear Forms}
\subsection{Stochastic Trace Estimator} \label{STE}
For trace estimation of large implicit matrices, we can use randomized estimator described by Hunchinson \cite{hutchinson1989stochastic, adams2018estimating}.
\begin{theorem}
    \label{Hunchinson}
    (Proposition 4.1 \cite{adams2018estimating}) Let ${\mathbf A} \in \mathbb{R}^{n\times n}$ be a square matrix and $\mathbf{x} \in \mathbb{R}^n$ be a random vector such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$. Then $\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{M}\mathbf{x}] = \mathrm{tr}(\mathbf{M})$.
\end{theorem}

\begin{proof}
    \begin{align*}
        \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}\mathbf{M}\mathbf{x}] &= \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{x}^{T}\mathbf{M}\mathbf{x})] \\
          &=  \mathbb{E}_{p(\mathbf{x})}[\mathrm{tr}(\mathbf{M}\mathbf{x}\mathbf{x}^{T})]  &\text{invariance to cyclic permutation} \\
          &= \mathrm{tr}(\mathbb{E}_{p(\mathbf{x})}[\mathbf{M}\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of trace}  \\
          &= \mathrm{tr}(\mathbf{M}\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}\mathbf{x}^{T}] ) &\text{linearity of expectation}  \\
          &= \mathrm{tr}(\mathbf{M})
    \end{align*}
\end{proof}

The requirement $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$ for the random vector  $\mathbf{x}$ can be easily satisfied. For example, paractical choices of $p(\mathbf{x})$ include Ramemacher or standard normal distributions with zero mean and one variance.

The following corollary holds.

\begin{corollary}
    \label{approxtrace}
    If $f(\mathbf{M}) \in \mathbb{R}^{n\times n}$ is a square matrix, and $\mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_{n_v} \in \mathbb{R}^n$ are random vectors drawn from a distribution $p(\mathbf{x})$ such that $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) =\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}f(\mathbf{M})\mathbf{x}] \approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} \mathbf{x}_i^{T}f(\mathbf{M})\mathbf{x}_i
    \end{align*}
\end{corollary}


Unit vectors $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ can be used instead of $\mathbf{x}_i$. We can rewrite the formula of Corollary \ref{approxtrace}.
\begin{theorem}
    \label{approxtraceUniformed}
    Let $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$ be a unit vector, then
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
    \end{align*}
\end{theorem}
\begin{proof}
    \begin{align*}
        \mathrm{tr}(f(\mathbf{M})) &\approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} \mathbf{x}_i^{T}f(\mathbf{M})\mathbf{x}_i \\
        &= \frac{1}{n_v}\sum_{i=0}^{n_v-1} \|\mathbf{x}_i\|_2^2 \cdot \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i \\
        &\approx \frac{1}{n_v}\sum_{i=0}^{n_v-1} n \cdot \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i &\text{$ \mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$}\\
        &= \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
    \end{align*}
\end{proof}

For Randemacher vectors, $\|\mathbf{x}_i\|_2^2$ is always $n$. For other random vectors, $\mathbb{E}_{p(\mathbf{x})}[\|\mathbf{x}_i\|_2^2$]  is equal to $n$ as long as $\mathbb{E}_{p(\mathbf{x})}[\mathbf{xx^T}] = \mathbf{I}$.

For accurate computation, $n_v$ should approaches infinity. However  using large $n_v$ is expensive. For practical purposes, we can use small $n_v$. The convergence rate of such estimator was studied by Avron and Toledo\cite{avron2011randomized}.


\subsection{Estimation of bilinear form $\mathbf{z}^T f(\mathbf{M}) \mathbf{z}$} \label{bilinear}
There are relationships between matrix, orthogonal polynomials, quadrature rules and the Lanczos algorithms. Gene H Golub and G\'erard Meurant compiled a book of previous research on such relationships and on estimation and bounds of bilinear form $\mathbf{u}^T f(\mathbf{M}) \mathbf{v}$ \cite{golub2009matrices}. In this paper, we focus on a special case that $\mathbf{u}=\mathbf{v}=\mathbf{z}$.

Firstly, the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ is reformed as a Riemann-Stieltjes integral and approximated by applying Gauss quadrature rule. Then, the Lanczos algorithm is applied for calculation.

The Riemann–Stieltjes integral is a generalization of the Riemann integral. It is defined to be the limit of a sum.
\begin{definition}
    A Riemann–Stieltjes integral of a real valued function $f$ of a real variable on the infinite interval $[a,b]$ with respect to a real function $\alpha$ is denoted by
    \[ \int_a^b f(x) d\alpha (x). \]
    This integral is defined to be the limit, as the length of the subinterval of the partition $\pi=\{a=x_0 < x_1 < ... < x_n = b\}$ goes to zero, of the approximating sum
    \[ \sum_{x_i \in \pi} f(c_i)(\alpha(x_{i+1}) - \alpha(x_i)).\]
    where $c_i$ is in the i-th subinterval $[x_i, x_{i+1}]$.
\end{definition}

Now, we consider the approximation of this integral by Gauss quadrature rule. This is a relation:

\begin{align}
    I[f] =\int_a^b f(x) d\alpha(x) = \sum_{k=0}^{s-1} w_k f(\theta_k) + R[f]
\end{align}

where $\{w_k\}$ are the unknowns weights and $\{\theta_k\}$ are the unknowns nodes. In the right-hand side, the sum $\sum_{k=0}^{s-1} w_k f(\theta_k)$ is the approximation of the Riemann-Stieltjes integral $I[f]$, and $R[f]$ is the remainder. If $R[p]=0$ for all polynomials $p$ of degree $d$ and $R[q] \neq 0$ for any polynomials $q$ of degree $d+1$, the rule is called {\it exact}.

The value of $R[f]$ is known \cite{golub2009matrices,stoer2013introduction}. If the measure $\alpha(x)$ is a positive nondecreasing function and $f \in C^{2s}[a,b]$, then
\begin{align}
    R[f] = \frac{f^{2s}(\xi)}{(2s)!} \int_a^b \left[\prod_{k=0}^{s-1}(x-\theta_k) \right]^2 d\alpha(x)
\end{align}
for some $\xi \in (a,b)$. Therefore, the Gauss rule is exact for polynomials $f$ of degree less than $2s$.

Finally, we can reform and compute the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ using the Gauss quadrature rule.
\begin{align*}
    \mathbf{z}^{T}f(\mathbf{M})\mathbf{z} &= \mathbf{z}^{T}\mathbf{Q}f(\mathbf{\Lambda})\mathbf{Q}^T\mathbf{z}\\
    &= \sum_{j=0}^{n-1}f(\mathbf{\lambda}_i)\mu^2_j \\
    &= \int_a^b f(t) d\mu(t) &\text{Riemann–Stieltjes integral}\\
    &\approx \sum_{k=0}^{s-1} \omega_k f(\theta_k) &\text{the Gauss quadrature rule}
\end{align*}
where $\mu_j = [\mathbf{Q}^T\mathbf{z}]_j$ and $\mu(t)$ is a piecewise constant function defined as
\begin{align*}
    \mu(t) = \begin{cases}
        0 & \text{if $t<a=\lambda_0$} \\
        \sum_{j=0}^{i-1} \mu_j^2 & \text{if $\lambda_{i-1} \leq t < \lambda_i, i=1,\cdots, n-1$} \\
        \sum_{j=0}^{n-1} \mu_j^2 & \text{if $b=\lambda_{n-1}\leq t$}
      \end{cases}
\end{align*}
and $\{\omega_k\}$ are the weights and $\{\theta_k\}$ are the nodes of the s-point Gauss quadrature.

One choise for computing the nodes and the weights of the Gauss quadrature rule is the Lanczos algorithm. Let $\mathbf{M}$ be a real symmetric matrix, $\mathbf{w}_0$ be an arbitrary starting unit-vector, and ${\mathbfcal K}$ be the Krylov subspace spanning vectors $\{\mathbf{w}_0, \mathbf{M}\mathbf{w}_0, \cdots, \mathbf{M}^{s-1}\mathbf{w}_0\}$.
The output of the Lanczos algorithm are an $n \times s$ matrix $\mathbf{W}$ and an $s \times s$ tridiagonal matrix $\mathbf{T}$, such that $\mathbf{W}^T\mathbf{M}\mathbf{W} = \mathbf{T}$. $\mathbf{W}$ has the orthonormal columns:
\begin{align*}
    w_0 : \text{an initial unit-vector}\\
    w_k = p_k(\mathbf{M})w_0,~  k=1,\cdots,s-1
\end{align*}
where $p_k$ are the Lanczos polynomials which are orthogonal with respect to the measure $\mu(t)$.

Now, the bilinear form $\mathbf{z}^{T}f(\mathbf{M})\mathbf{z}$ is reformed as
\begin{align}
    &\mathbf{z}^{T}f(\mathbf{M})\mathbf{z} \approx  \sum_{k=0}^{s-1} \tau_k^2 f(\theta_k)\\
    &\tau_k = \mathbf{Y}_{0,k} = \mathbf{e}_1^T \mathbf{y}_k, ~ \theta_k= \Theta_{k,k}, ~ \mathbf{T}=\mathbf{Y}\mathbf{\Theta}\mathbf{Y}^T
\end{align}

\subsection{Stochastic Lanczos Quadrature(SLQ)}
There is a method called stochastic Lanczos quadrature(SLQ) \cite{ubaru2017fast}, for approximate computing of the trace of functions of large matrices. Firstly, the stochastic trace estimator is used for approximating the trace. Next, the estimation method we discussed in the section \ref{bilinear} is applied.

The stochastic trace estimator has been explained in the previous section.
\begin{align}
    \mathrm{tr}(f(\mathbf{M})) =\mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}f(\mathbf{M})\mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i
\end{align}

Now, we need to compute $\mathbf{z}_i^{T}f(\mathbf{M})\mathbf{z}_i$. By applying (2.1) to the stochastic trace estimator (2.3), we get Stochastic Lanczos Quadrature estimator:
\begin{align}
    \mathrm{tr}(f(\mathbf{M})) \approx \frac{n}{n_v} \sum_{i=0}^{n_v-1}\left(\sum_{k=0}^{s-1} (\tau_k^i)^2 f(\theta_k^i) \right)
\end{align}



\section{Spectral Distribution}
In the field of condensed matter physics, the density of states is defined as the distribution of eigenvalues. Since all eigenvalues need to be calculated for the exact solution, the computational complexity becomes $O(n^3)$ when the number of vertices is n. Therefore, it does not scale well for large-scale networks with millions or billions of vertices. Recently, Kun Dong et al. have developed an approximation algorithm for finding the distribution of eigenvalues in a network \cite{dong2019network}. This has made it possible to obtain the distribution of eigenvalues for large graphs in a practical time.

First, we define DOS as the spectrum of the entire network. Next, we define LDOS as the local spectrum, and define what the equation looks like when we consider the spectrum of each vertex as its special case.
\begin{definition}
    Let ${\mathbf M} \in \mathbb{R}^{n\times n}$ be any real symmetric matrix. The spectral density, or density of states (DOS) is
    \begin{align}
        \mu(\lambda) = \frac{1}{n}\sum_{i=0}^{n-1} \delta(\lambda - \lambda_i), ~ \int f(\lambda)\mu(\lambda)= \mathrm{tr}(f({\mathbf M}))
    \end{align}
    where $\delta$ is the Dirac delta function and $f$ is any analytic test function.
\end{definition}
\begin{definition}
    For any vector $u \in \mathbb{R}^n$, the local density of states (LDOS) is
    \begin{align}
        \mu(\lambda ; u) = \sum_{i=0}^{n-1} |u^T q_i|^2 \delta(\lambda - \lambda_i), ~ \int f(\lambda) \mu(\lambda; u) = u^T f({\mathbf M}) u
    \end{align}
    where $Q=[q_0, ..., q_{n-1}]$ is orthogonal.
    Especially, when we consider the case $u=e_k$ where $e_k$ is the $k$-th standard basis vector, the pointwise density of states (PDOS) is
    \begin{align}
        \mu_k(\lambda) = \sum_{i=0}^{n-1} |e_k^T q_i|^2 \delta(\lambda - \lambda_i) = \sum_{i=0}^{n-1} |q_i(k)|^2 \delta(\lambda - \lambda_i), ~ \int f(\lambda) \mu_k(\lambda) = e_k^T f({\mathbf M}) e_k
    \end{align}
    where $|q_i(k)|$ is the magnitude of the weight for a $k$-th vertex in the $i$-th eigenvector.
\end{definition}



\chapter{Gallery and Applications of Spectral Distributions}
\section{Quantitative Evaluation}
\section{Experiments}
\subsection{Gallery of DOS/PDOS}
\subsection{Propaties of Graphs with Similar DOS/PDOS}

\chapter{the Sum of Powers of Eigenvalues of Network (SPENet)}
Our main purpose is computing sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$ which we call SPENet. We propose two approximation algorithms (STE and SLQ) based on the stochastic trace estimator and the stochastic Lanczos quadrature. For naive algorithms, the computational complexity of SPENet using full eigendecomposition is $O(n^3)$, while for STE algorithm it is $O(m k n_v)$ and for SLQ algorithm it is $O((ms + ns^2) n_v)$.

Since the laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ have zero eigenvalues, we are interested in the case that index of power $k$ is positive. When we consider the case that ${\mathbf M} = {\mathbf A}$, $k$ must be positive integer or zero because ${\mathbf A}$ have negative eigenvalues.


\section{Algorithms}
\subsection{STE algorithm}
The first estimation method for SPENet is based on the stochastic trace estimator(STE), which we apply in our setting for the trace of power of matrix $f({\mathbf M})={\mathbf M}^k$. We are interested in the case where ${\mathbf M} = {\mathbf A}, {\mathbf L}, \text{or} {\mathbfcal L}$. In this algorithm, the index of power $k$ must be positve integer.

By using theorem \ref{power_eigenvalues} and \ref{approxtraceUniformed}, we can apply the stochastic trace estimator for computing sum of powers of k-th eigenvalues.
\begin{align}
    \sum_{i=0}^{n-1} \lambda_i^k &= \mathrm{tr}({\mathbf M}^k)  \nonumber\\
    &= \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}{\mathbf M}^k \mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i \\
    &= \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T} \underbrace{{\mathbf M}{\mathbf M} \cdots \cdots {\mathbf M}}_{k \text{ times}} \mathbf{z}_i
\end{align}

The product of two matrices costs $O(n^3)$, but the product of a vector and a sparse matrix costs only $O(m)$. Therefore, when calculating $\mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i$, we just calculate the product of a vector and a matrix k times, and the inner product of a vector once from the left. From the above, the cost of the STE algorithm will be $O(mkn_v)$.


\subsection{SLQ algorithm}
The second estimation method for SPENet is based on the stochastic Lanczos quadrature(SLQ), which we apply in our setting for the trace of power of matrix $f({\mathbf M})={\mathbf M}^k$. We are interested in the case where ${\mathbf M} = {\mathbf A}, {\mathbf L}, \text{or} {\mathbfcal L}$. In this algorithm, the index of power $k$ must be positve real number.

By using theorem 1 and (2.4), we can estimate SPENet.
\begin{align}
    \sum_{i=0}^{n-1} \lambda_i^k &= \mathrm{tr}({\mathbf M}^k)  \nonumber\\
    &= \mathbb{E}_{p(\mathbf{x})}[\mathbf{x}^{T}{\mathbf M}^k \mathbf{x}] \approx \frac{n}{n_v}\sum_{i=0}^{n_v-1} \mathbf{z}_i^{T}{\mathbf M}^k\mathbf{z}_i \nonumber\\
    &\approx \frac{n}{n_v} \sum_{i=0}^{n_v-1}\left(\sum_{j=0}^{s-1} (\tau_j^i)^2 (\theta_j^i)^k \right)
\end{align}
Since we apply s-step Lanczos algorithm for $\tau_k^i$ and $\theta_k^i$, the orthogonalization cost inside the Lanczos algorithm is $O(ns^2)$ and the cost of the product of a vector and a matrix s times in the Lanczos algorithm is $O(ms)$. From the above, all the computational cost of SLQ algorithm is $O((ms+ns^2)n_v)$.


\begin{algorithm}
    \caption{STE algorithm}
    \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE a real symmetric matrix ${\mathbf M} \in {\mathbb R}^{n\times n}$, a positive integer $k$, and $n_v$.
    \ENSURE  Approximate sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$
     \FOR {$i = 0$ to $n_v-1$}
     \STATE Generate a random vector $\mathbf{x}_i$ with the standard normal distribution.
     \STATE Form unit vector $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$.
     \STATE $\mathbf{v}_i^T \leftarrow \mathbf{z}_i^T $.
        \FOR {$j = 0$ to $k-1$}
        \STATE $\mathbf{v}_i^T \leftarrow \mathbf{v}_i^T {\mathbf M}$.
        \ENDFOR
     \STATE $\Gamma \leftarrow \Gamma + \mathbf{v}_i^T \mathbf{z}_i$
     \ENDFOR
    \STATE $\Gamma \leftarrow \frac{n}{n_v}\Gamma$
    \RETURN $\Gamma$
    \end{algorithmic}
\end{algorithm}

\begin{algorithm}
    \caption{SLQ algorithm}
    \begin{algorithmic}[1]
    \renewcommand{\algorithmicrequire}{\textbf{Input:}}
    \renewcommand{\algorithmicensure}{\textbf{Output:}}
    \REQUIRE a real symmetric matrix ${\mathbf M} \in {\mathbb R}^{n\times n}$, a positive real number $k$, and $n_v$.
    \ENSURE  Approximate sum of k-th powers of eigenvalues $\sum_{i=0}^{n-1} \lambda_i^k$
     \FOR {$i = 0$ to $n_v-1$}
     \STATE Generate a random vector $\mathbf{x}_i$ with the standard normal distribution.
     \STATE Form unit vector $\mathbf{z}_i = \frac{\mathbf{x}_i}{\|\mathbf{x}_i\|_2}$.
     \STATE $T \leftarrow \text{Lanczos($\mathbf{M}$,$\mathbf{z}_i$, s)}$.
     \STATE $[\mathbf{Y}, \mathbf{\Theta}] \leftarrow \text{eig}(T)$
     \STATE Compute $\tau_j = [e_1^T y_j]$ for $j=0, ..., s-1$
     \STATE $\Gamma \leftarrow \Gamma + \sum_{j=0}^{s-1} (\tau_j)^2 (\theta_j)^k$
     \ENDFOR
    \STATE $\Gamma \leftarrow \frac{n}{n_v}\Gamma$
    \RETURN $\Gamma$
    \end{algorithmic}
\end{algorithm}


\section{Conditions of $k$}
In SLQ algorithm, we can generalize the index of power $k$. If ${\mathbf M}$ is positive definite, we can use not only a positive integer but also a positive real number as $k$. This is because that $f(x)=x^k$ is analytic inside a closed interval $[\lambda_0, \lambda_{n-1}]$. However the function $f(x)=x^k$ have a singularity at zero and the laplacian matrix ${\mathbf L}$ and the normalized laplacian matrix ${\mathbfcal L}$ are positive semi-definite matrix, which have $\lambda_0 = 0$.

To overcome the issue mentioned above, we can use {\it shifting the spectrum} proposed by Shashanka Ubaru, Jie Chen, and Yousef Saad\cite{ubaru2017fast}. The idea is shifting the eigenvalues by replacing ${\mathbf M}$ with ${\mathbf M}+\delta{\mathbf I}$. The shifted matrix have the eigenvalues in the interval $[\lambda_0 + \delta, \lambda_{n-1}+ \delta]$. Hence we can obtaing the approximation error bounds of SLQ algorithm and find that SLQ algorithm is practically useful.

If $k$ is a positive real number and ${\mathbf M}={\mathbf A}$, the SPENet $\sum_{i=0}^{n-1} \lambda_i^k$ is undefined because ${\mathbf A}$ have negative eigenvalues.

\begin{table}[htb]
    \begin{center}
      \begin{tabular}{l| c c c }
       & ${\mathbf A}$ & ${\mathbf L}$ & ${\mathbfcal L}$ \\ \hline
        $k\in \mathbb{Z}_{\geq 0}$ & STE or SLQ  & STE or SLQ  & STE or SLQ \\
        $k\in \mathbb{R}_{\geq 0}$&  undefined  & SLQ  &  SLQ \\
      \end{tabular}
      \caption{Conditions under which the algorithms can be used}
      \label{tab:notion}
    \end{center}
  \end{table}

\section{Error bounds}
Suppose $k$ is a positive integer and Lanczos step size is $s$. When $2s-1$ is greater or equal to the degree of $f(x)$, the error of Gauss quadrature rule is zero.

\section{Experiments}
We evaluate computational time and variance of SPENet using STE and SLQ algorithm. Additionally, we evaluate its relative error against the exact computation of the eigenvalues where allowed by the graph size. We perform our experiments averating 10 times for all experiments unless stated otherwise.

We use 40 real-world small networks\cite{nr}.
\subsection{Parameter Settings}

\subsection{Approximation Accuracy}
\subsection{Parameter Sensitivity}

\chapter{Conclusion}

%-------------------
\bibliographystyle{plain} % 参考文献
\bibliography{myref} %
%-------------------
\end{document}
